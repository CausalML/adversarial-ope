{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import itertools\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "from stable_baselines3 import DQN, PPO\n",
    "\n",
    "from gym_sepsis.envs.sepsis_env import SepsisEnv\n",
    "from environments.sepsis_env_wrapper import SepsisEnvWrapper\n",
    "from policies.sb3_policy import SB3Policy\n",
    "from utils.offline_dataset import OfflineRLDataset\n",
    "from models.fnn_nuisance_model import FeedForwardNuisanceModel\n",
    "from models.large_a_fnn_nuisance_model import LargeAFeedForwardNuisanceModel\n",
    "from models.fnn_critic import FeedForwardCritic\n",
    "from learners.robust_fqi_learner import RobustFQILearner\n",
    "from learners.iterative_sieve_critic import IterativeSieveLearner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('configs/sepsis_config.json') as f:\n",
    "    sepsis_config = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train PPO policy over large number of timesteps for behavior policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_env = SepsisEnv()\n",
    "env = SepsisEnvWrapper(base_env=base_env, s_init_idx=0)\n",
    "\n",
    "num_a = env.get_num_a()\n",
    "state_dim = env.get_s_dim()\n",
    "\n",
    "print(f'Num actions: {num_a}')\n",
    "print(f'State dimension: {state_dim}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train a policy using PPO\n",
    "\n",
    "ppo_kwargs = sepsis_config['ppo_model_kwargs']\n",
    "ppo_model = PPO(\n",
    "    'MlpPolicy', env,\n",
    "    gamma=sepsis_config['gamma'],\n",
    "    **ppo_kwargs\n",
    ")\n",
    "ppo_total_timesteps = sepsis_config['ppo_num_updates'] * ppo_kwargs['n_steps']\n",
    "ppo_model.learn(total_timesteps=ppo_total_timesteps, progress_bar=True)\n",
    "\n",
    "ppo_model.save(sepsis_config['ppo_model_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train DQN over relatively small number of timesteps for evaluation policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train evaluation model with DQN over smaller number of iterations\n",
    "\n",
    "dqn_model = DQN(\n",
    "    'MlpPolicy', env,\n",
    "    gamma=sepsis_config['gamma'],\n",
    "    **sepsis_config['dqn_model_kwargs']\n",
    ")\n",
    "dqn_model.learn(total_timesteps=sepsis_config['dqn_total_timesteps'], progress_bar=True)\n",
    "\n",
    "dqn_model.save(sepsis_config['dqn_model_path'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Offline Dataset using Behavioral (PPO) Policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build offline datasets\n",
    "\n",
    "pi_b = SB3Policy(env, model=PPO.load(sepsis_config['ppo_model_path']))\n",
    "pi_e = SB3Policy(env, model=DQN.load(sepsis_config['dqn_model_path']))\n",
    "pi_e_name = sepsis_config['pi_e_name']\n",
    "\n",
    "\n",
    "dataset = OfflineRLDataset()\n",
    "burn_in = sepsis_config['dataset_burn_in']\n",
    "num_sample = sepsis_config['dataset_num_samples']\n",
    "thin = sepsis_config['dataset_thin']\n",
    "dataset.sample_new_trajectory(\n",
    "    env=env,\n",
    "    pi=pi_b,\n",
    "    burn_in=burn_in,\n",
    "    num_sample=num_sample,\n",
    "    thin=thin\n",
    ")\n",
    "\n",
    "test_dataset = OfflineRLDataset()\n",
    "test_dataset.sample_new_trajectory(\n",
    "    env=env,\n",
    "    pi=pi_b,\n",
    "    burn_in=burn_in,\n",
    "    num_sample=num_sample,\n",
    "    thin=thin\n",
    ")\n",
    "\n",
    "dataset.apply_eval_policy(pi_e_name, pi_e)\n",
    "test_dataset.apply_eval_policy(pi_e_name, pi_e)\n",
    "\n",
    "dataset.save_dataset(sepsis_config[\"train_dataset_path\"])\n",
    "test_dataset.save_dataset(sepsis_config[\"test_dataset_path\"])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "adversarial-rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
